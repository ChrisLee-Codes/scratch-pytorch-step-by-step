{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 线性模型和全连接神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本节，我们会介绍一种最基本的神经网络模型，全连接神经网络，其实在之前的几节，我们所实现的就是全连接神经网络，但是在本节我们会用pytorch的风格实现Module类，实现更方便的构建模型的函数，同时我们会介绍何为欠拟合，过拟合，以及一些消除欠拟合过拟合的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们首先把我们在张量这几节中实现的内容放到一个py文件中，方便我们之后的调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mytorch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先实现Module类，这个类会是所有模型的夫类，主要实现了模型结构的定义，参数的初始化，同时记录和追踪模型的参数变化，当你没有定义模型前向传播的方法的时候，会报错，因为python中没有借口，我们用raise来表示，让每个子类保证实现了forward方法，同时我们希望这个类的实例被调用时，自带运行forward来求解，所以实现了\\_\\_call\\_\\_方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self._parameters = {}\n",
    "        self._modules = {}\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "    def __call__(self, *input):\n",
    "        return self.forward(*input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来实现一个简单的线性模型，测试我们的Module类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.w = Tensor.from_numpy(np.random.rand(in_features, out_features))\n",
    "        self.b = Tensor.from_numpy(np.random.rand(1, out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x*self.w+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 10\n",
    "out_features = 5\n",
    "\n",
    "linear_layer = Linear(in_features, out_features)\n",
    "x = Tensor.from_numpy(np.random.rand(1, in_features))\n",
    "output = linear_layer(x)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们希望实现named_parameters方法，这个方法会记录这个模型的各层的形状和数值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\_\\_setattr\\_\\_ 是一个特殊方法（或称魔术方法）在 Python 中，用于拦截对对象属性的赋值操作。当你试图给对象的一个属性赋值时，这个方法会被自动调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositiveNumber:\n",
    "    def __init__(self, initial_value):\n",
    "        self.value = initial_value\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if name == 'value' and value < 0:\n",
    "            raise ValueError(\"Value must be positive\")\n",
    "        object.__setattr__(self, name, value)\n",
    "\n",
    "obj = PositiveNumber(5)\n",
    "obj.value = -10  # 这会触发 ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们会用\\_\\_setattr\\_\\_及时更新parameters和modules列表，然后通过调用named_parameters打印参数列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " object.\\_\\_setattr\\_\\_(self, name, value) 用于在对象中设置一个属性。在 Python 中，当我们为一个对象设置属性时，如 self.attribute = value，实际上是在调用该对象的 \\_\\_setattr\\_\\_ 方法。但在我们自定义的 \\_\\_setattr\\_\\_ 方法中，如果直接使用 self.attribute = value，这会再次触发 \\_\\_setattr\\_\\_，从而导致无限递归。为了避免这种情况，在自定义的 \\_\\_setattr\\_\\_ 方法中设置属性时，我们应该使用基类（在这种情况下是 object 类）的 \\_\\_setattr\\_\\_ 方法。这样做可以直接在对象上设置属性，而不会再次触发 \\_\\_setattr\\_\\_。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self._parameters = {}\n",
    "        self._modules = {}\n",
    "\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, *input):\n",
    "        return self.forward(*input)\n",
    "\n",
    "    def named_parameters(self, memo=None, prefix=''):\n",
    "        if memo is None:\n",
    "            memo = set()\n",
    "\n",
    "        for name, param in self._parameters.items():\n",
    "            if param not in memo:\n",
    "                memo.add(param)\n",
    "                yield prefix + name, param\n",
    "\n",
    "        for name, mod in self._modules.items():\n",
    "            submodule_prefix = prefix + name + '.'\n",
    "            for name, param in mod.named_parameters(memo, submodule_prefix):\n",
    "                yield name, param\n",
    "\n",
    "    def add_module(self, name, module):\n",
    "        if not isinstance(module, Module) and module is not None:\n",
    "            raise TypeError(\"{} is not a Module subclass\".format(type(module)))\n",
    "        self._modules[name] = module\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, Tensor):\n",
    "            object.__setattr__(self, name, value)  # 先设置属性\n",
    "            self._parameters[name] = value          # 然后添加到参数字典中\n",
    "        elif isinstance(value, Module):\n",
    "            self.add_module(name, value)\n",
    "        else:\n",
    "            object.__setattr__(self, name, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们再来测试一下Linear这个类的参数列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.w = Tensor.from_numpy(np.random.rand(in_features, out_features))\n",
    "        self.b = Tensor.from_numpy(np.random.rand(1, out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x*self.w+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, parameter in linear_layer.named_parameters():\n",
    "    print(name, parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linear_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们完成\\_\\_repr\\_\\_方法，来自动打印模型的网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self._parameters = {}\n",
    "        self._modules = {}\n",
    "\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, *input):\n",
    "        return self.forward(*input)\n",
    "\n",
    "    def named_parameters(self, memo=None, prefix=''):\n",
    "        if memo is None:\n",
    "            memo = set()\n",
    "\n",
    "        for name, param in self._parameters.items():\n",
    "            if param not in memo:\n",
    "                memo.add(param)\n",
    "                yield prefix + name, param\n",
    "\n",
    "        for name, mod in self._modules.items():\n",
    "            submodule_prefix = prefix + name + '.'\n",
    "            for name, param in mod.named_parameters(memo, submodule_prefix):\n",
    "                yield name, param\n",
    "\n",
    "    def add_module(self, name, module):\n",
    "        if not isinstance(module, Module) and module is not None:\n",
    "            raise TypeError(\"{} is not a Module subclass\".format(type(module)))\n",
    "        self._modules[name] = module\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, Tensor):\n",
    "            object.__setattr__(self, name, value)  # 先设置属性\n",
    "            self._parameters[name] = value          # 然后添加到参数字典中\n",
    "        elif isinstance(value, Module):\n",
    "            object.__setattr__(self, name, value)\n",
    "            self.add_module(name, value)\n",
    "        else:\n",
    "            object.__setattr__(self, name, value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        lines = [self.__class__.__name__ + '(']\n",
    "        for name, module in self._modules.items():\n",
    "            mod_str = repr(module).replace('\\n', '\\n  ')\n",
    "            lines.append(f\"  ({name}): {mod_str}\")\n",
    "        lines.append(')')\n",
    "        return '\\n'.join(lines)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们再对Linear类进行简单的修改，让其变成我们的标准模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, bias = True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.w = Tensor.from_numpy(np.random.rand(in_features, out_features))\n",
    "        if bias:\n",
    "            self.b = Tensor.from_numpy(np.random.rand(1, out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x*self.w+self.b if self.bias else x*self.w\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Linear(in_features={self.in_features}, out_features={self.out_features}, bias={self.bias})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Linear(5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l(Tensor.from_numpy(np.random.rand(1, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = Linear(5,6)\n",
    "        self.l2 = Linear(6,8)\n",
    "        self.l3 = Linear(8,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = x.relu()\n",
    "        x = self.l2(x)\n",
    "        x = x.relu()\n",
    "        x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(Tensor.from_numpy(np.random.rand(1, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，让我们摩饭pytorch风格，实现MSELoss类做一个收尾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        loss = 0\n",
    "        for i in range(len(target)):\n",
    "            loss += (pred[i] - target[i]) ** 2\n",
    "        return loss/len(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们实现随机批次类，我们会在每个epoch开始的时候随机打乱整个数据集，再按批次大小从中取数，知道取完再开始下一个循环的随机打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler:\n",
    "    def __init__(self, train_x, train_y, batch_size):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(self.train_x))\n",
    "        np.random.shuffle(self.indices)  # Shuffle at the start\n",
    "        self.current_index = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        if self.current_index + self.batch_size > len(self.train_x):\n",
    "            # Reshuffle the indices and reset the current index\n",
    "            np.random.shuffle(self.indices)\n",
    "            self.current_index = 0\n",
    "\n",
    "        batch_indices = self.indices[self.current_index:self.current_index + self.batch_size]\n",
    "        self.current_index += self.batch_size\n",
    "        return self.train_x[batch_indices], self.train_y[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，让我们简单测试一下我们的程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return 4*x*x - 5\n",
    "\n",
    "import numpy as np\n",
    "x_values = np.linspace(-10, 10, 100)\n",
    "y_values = func(x_values)\n",
    "\n",
    "# Randomly choosing training data\n",
    "random_indices = np.random.choice(len(x_values), size=80, replace=False)\n",
    "train_x = x_values[random_indices]\n",
    "train_y = y_values[random_indices]\n",
    "\n",
    "# The remaining data can be considered as the testing dataset\n",
    "test_x = np.delete(x_values, random_indices)\n",
    "test_y = np.delete(y_values, random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = Linear(1,10)\n",
    "        self.l2 = Linear(10,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = x.relu()\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30000\n",
    "t = 0\n",
    "mse_loss = MSELoss()\n",
    "for i in range(epoch):\n",
    "    loss = mse_loss([net(x) for x in train_x], train_y)\n",
    "    loss.backward()\n",
    "    t = loss.adam_opt(t, learning_rate=0.001)\n",
    "    if i % 100 == 0:\n",
    "        print(f'Epoch {i}/{epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw(forward_func, test_x, test_y):\n",
    "    # 创建绘图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # 绘制 forward 函数的曲线\n",
    "    line_x = np.linspace(min(test_x), max(test_x), 100)\n",
    "    line_y = [forward_func(x).data[0][0] for x in line_x]\n",
    "    plt.plot(line_x, line_y, label=\"Forward Function\", color='blue')\n",
    "\n",
    "    # 绘制测试集的点\n",
    "    plt.scatter(test_x, test_y, label=\"Test Data\", color='red')\n",
    "\n",
    "    # 添加图例和标签\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Forward Function vs Test Data')\n",
    "    plt.legend()\n",
    "\n",
    "    # 显示图形\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw(net, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30000\n",
    "batch_size = 80\n",
    "batch_sampler = BatchSampler(train_x, train_y, batch_size)\n",
    "mse_loss = MSELoss()\n",
    "t = 0\n",
    "for i in range(epoch):\n",
    "    loss_sum = 0\n",
    "    for _ in range(len(train_y)//5):\n",
    "        batch_train_x, batch_train_y = batch_sampler.next_batch()\n",
    "        pred = [net(x) for x in batch_train_x]\n",
    "        loss = mse_loss(pred, batch_train_y)\n",
    "        loss_sum += loss\n",
    "        loss.backward()\n",
    "        loss.adam_opt(t, learning_rate=0.001)\n",
    "    if i % 100 == 0:\n",
    "        print(f'Epoch {i}/{epoch}, Loss: {loss_sum}')\n",
    "    t += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
