{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3神经元（下）\n",
    "本节是神经元的最后一部分，我们会补充一些上一节的神经元改进方案，同时介绍一些新的梯度下降迭代策略，相信细心的读者以及发现，在我们上一节的迭代策略中，我们不一定会找到全局最优点，而可能会陷入局部最优点，因此我们会介绍新的梯度迭代策略，来实现梯度下降策略。<br>\n",
    "同时读者也会发现在神经网络中，矩阵运算是如此普遍，但是上一节的Tensor类只支持传入整型或浮点型，我们会重写Tensor类，让其支持对矩阵的计算和反向传播。<br>\n",
    "本节还会实现一个简单的模型可视化模块，读者可以通过这个模块知道自己构造的模型结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们将Tensor转换乘numpy形式计算，来支持矩阵运算，其实矩阵求导和正常的求导没有本质区别，在上一节中我们设置了1*10个神经元作为输入层的下一层，然后输出层用了10*1的神经元，我们的计算结果实际上就是矩阵乘积的求导，可见在神经网络中，矩阵求导更多体现在对于复杂网络结构的简化表示，并没有变化原来的计算规则。若想了解详细的矩阵求导公式和推导过程，可参考这篇博客：https://zhuanlan.zhihu.com/p/273729929 <br>\n",
    "为了方便我们使用，我们还添加了类方法(类似其他语言的静态方法)方便numpy和Tensor的互换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def from_numpy(n):\n",
    "    return \n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, _prev=(), trainable=True):\n",
    "        self.data = np.array(data)\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_prev)\n",
    "        self.trainable = trainable\n",
    "\n",
    "    @classmethod\n",
    "    def from_numpy(cls, array, trainable=True):\n",
    "        return cls(array, trainable=trainable)\n",
    "    \n",
    "    def to_numpy(self):\n",
    "        return self.data\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            other = Tensor(other, trainable=False)\n",
    "        out_data = self.data + other.data\n",
    "        out = Tensor(out_data, (self, other))\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            other = Tensor(other, trainable=False)\n",
    "        out_data = self.data - other.data\n",
    "        out = Tensor(out_data, (self, other))\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad -= 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return (-self).__add__(other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        out_data = -self.data\n",
    "        neg_tensor = Tensor(out_data, (self,))\n",
    "        def _backward():\n",
    "            self.grad -= 1.0 * neg_tensor.grad  \n",
    "        neg_tensor._backward = _backward\n",
    "        return neg_tensor\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            other = Tensor(other, trainable=False)\n",
    "        out_data = np.dot(self.data, other.data)\n",
    "        out = Tensor(out_data, (self, other))\n",
    "        def _backward():\n",
    "            self.grad += np.dot(other.data.T, out.grad)\n",
    "            other.grad += np.dot(self.data.T, out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, (int, float)):\n",
    "            other = Tensor(other, trainable=False)\n",
    "        out_data = np.true_divide(self.data, other.data)\n",
    "        out = Tensor(out_data, (self, other))\n",
    "        def _backward():\n",
    "            self.grad += np.true_divide(1, other.data) * out.grad\n",
    "            other.grad -= np.true_divide(self.data, np.square(other.data)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, power):\n",
    "        out_data = np.power(self.data, power)\n",
    "        out = Tensor(out_data, (self,))\n",
    "        def _backward():\n",
    "            self.grad += power * (np.power(self.data, power - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = np.tanh(x)\n",
    "        out = Tensor(t, (self, ))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        x = self.data\n",
    "        s = 1 / (1 + np.exp(-x))\n",
    "        out = Tensor(s, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (s * (1 - s)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        x = self.data\n",
    "        r = np.maximum(0, x)\n",
    "        out = Tensor(r, (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (x > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def gradient_descent_opt(self, learning_rate=0.001, grad_zero=True):\n",
    "        for v in self.visited:\n",
    "            if v.trainable:\n",
    "                v.data -= learning_rate * v.grad\n",
    "            if grad_zero:\n",
    "                v.grad = 0\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        self.visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in self.visited:\n",
    "                self.visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, trainable={self.trainable})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来做个简单的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor Multiplication:\n",
      "[[ 1  3  2]\n",
      " [ 3  9  6]\n",
      " [ 5 15 10]]\n"
     ]
    }
   ],
   "source": [
    "# 测试代码\n",
    "# 创建两个矩阵\n",
    "tensor1 = Tensor([[1], [3], [5]])\n",
    "tensor2 = Tensor([[1, 3, 2]])\n",
    "\n",
    "# 矩阵相乘\n",
    "result_mul = tensor1 * tensor2\n",
    "print(\"\\nTensor Multiplication:\")\n",
    "print(result_mul.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们再用现在版本的神经元实现我们上一节二阶函数的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return 4*x*x - 5\n",
    "\n",
    "import numpy as np\n",
    "x_values = np.linspace(-10, 10, 30)\n",
    "y_values = func(x_values)\n",
    "\n",
    "# Randomly choosing training data\n",
    "random_indices = np.random.choice(len(x_values), size=20, replace=False)\n",
    "train_x = x_values[random_indices]\n",
    "train_y = y_values[random_indices]\n",
    "\n",
    "# The remaining data can be considered as the testing dataset\n",
    "test_x = np.delete(x_values, random_indices)\n",
    "test_y = np.delete(y_values, random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "w1 = Tensor.from_numpy(np.random.rand(1, 10))\n",
    "b1 = Tensor.from_numpy(np.random.rand(1, 10))\n",
    "w2 = Tensor.from_numpy(np.random.rand(10, 1))\n",
    "b2 = Tensor(random.random())\n",
    "epoch = 5000\n",
    "def forward(x):\n",
    "    x = x*w1+b1\n",
    "    x = x.relu()\n",
    "    x = x*w2+b2\n",
    "    return x\n",
    "for i in range(epoch):\n",
    "    loss = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节所实现的神经元类，并不会是神经元的最终版本，在后续介绍深度学习的经典模型的章节，我们会继续补充神经元类"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
