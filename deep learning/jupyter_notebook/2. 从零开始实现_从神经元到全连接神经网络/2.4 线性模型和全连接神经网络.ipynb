{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 线性模型和全连接神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本节，我们会介绍一种最基本的神经网络模型，全连接神经网络，其实在之前的几节，我们所实现的就是全连接神经网络，但是在本节我们会用pytorch的风格实现Module类，实现更方便的构建模型的函数，同时我们会介绍何为欠拟合，过拟合，以及一些消除欠拟合过拟合的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们首先把我们在张量这几节中实现的内容放到一个py文件中，方便我们之后的调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mytorch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先实现Module类，这个类会是所有模型的夫类，主要实现了模型结构的定义，参数的初始化，同时记录和追踪模型的参数变化，当你没有定义模型前向传播的方法的时候，会报错，因为python中没有借口，我们用raise来表示，让每个子类保证实现了forward方法，同时我们希望这个类的实例被调用时，自带运行forward来求解，所以实现了\\_\\_call\\_\\_方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self._parameters = {}\n",
    "        self._modules = {}\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "    def __call__(self, *input):\n",
    "        return self.forward(*input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来实现一个简单的线性模型，测试我们的Module类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.w = Tensor.from_numpy(np.random.rand(in_features, out_features))\n",
    "        self.b = Tensor.from_numpy(np.random.rand(1, out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x*self.w+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[[3.42376874 3.12571958 2.25732465 2.44523259 2.78703212]], grad=[[0. 0. 0. 0. 0.]], trainable=True)\n"
     ]
    }
   ],
   "source": [
    "in_features = 10\n",
    "out_features = 5\n",
    "\n",
    "linear_layer = Linear(in_features, out_features)\n",
    "x = Tensor.from_numpy(np.random.rand(1, in_features))\n",
    "output = linear_layer(x)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们希望实现named_parameters方法，这个方法会记录这个模型的各层的形状和数值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\_\\_setattr\\_\\_ 是一个特殊方法（或称魔术方法）在 Python 中，用于拦截对对象属性的赋值操作。当你试图给对象的一个属性赋值时，这个方法会被自动调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Value must be positive",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[1;32m     10\u001b[0m obj \u001b[38;5;241m=\u001b[39m PositiveNumber(\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m obj\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m\n",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m, in \u001b[0;36mPositiveNumber.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m----> 7\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue must be positive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n",
      "\u001b[0;31mValueError\u001b[0m: Value must be positive"
     ]
    }
   ],
   "source": [
    "class PositiveNumber:\n",
    "    def __init__(self, initial_value):\n",
    "        self.value = initial_value\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if name == 'value' and value < 0:\n",
    "            raise ValueError(\"Value must be positive\")\n",
    "        object.__setattr__(self, name, value)\n",
    "\n",
    "obj = PositiveNumber(5)\n",
    "obj.value = -10  # 这会触发 ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们会用\\_\\_setattr\\_\\_及时更新parameters和modules列表，然后通过调用named_parameters打印参数列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " object.\\_\\_setattr\\_\\_(self, name, value) 用于在对象中设置一个属性。在 Python 中，当我们为一个对象设置属性时，如 self.attribute = value，实际上是在调用该对象的 \\_\\_setattr\\_\\_ 方法。但在我们自定义的 \\_\\_setattr\\_\\_ 方法中，如果直接使用 self.attribute = value，这会再次触发 \\_\\_setattr\\_\\_，从而导致无限递归。为了避免这种情况，在自定义的 \\_\\_setattr\\_\\_ 方法中设置属性时，我们应该使用基类（在这种情况下是 object 类）的 \\_\\_setattr\\_\\_ 方法。这样做可以直接在对象上设置属性，而不会再次触发 \\_\\_setattr\\_\\_。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self._parameters = {}\n",
    "        self._modules = {}\n",
    "\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, *input):\n",
    "        return self.forward(*input)\n",
    "\n",
    "    def named_parameters(self, memo=None, prefix=''):\n",
    "        if memo is None:\n",
    "            memo = set()\n",
    "\n",
    "        for name, param in self._parameters.items():\n",
    "            if param not in memo:\n",
    "                memo.add(param)\n",
    "                yield prefix + name, param\n",
    "\n",
    "        for name, mod in self._modules.items():\n",
    "            submodule_prefix = prefix + name + '.'\n",
    "            for name, param in mod.named_parameters(memo, submodule_prefix):\n",
    "                yield name, param\n",
    "\n",
    "    def add_module(self, name, module):\n",
    "        if not isinstance(module, Module) and module is not None:\n",
    "            raise TypeError(\"{} is not a Module subclass\".format(type(module)))\n",
    "        self._modules[name] = module\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, Tensor):\n",
    "            object.__setattr__(self, name, value)  # 先设置属性\n",
    "            self._parameters[name] = value          # 然后添加到参数字典中\n",
    "        elif isinstance(value, Module):\n",
    "            self.add_module(name, value)\n",
    "        else:\n",
    "            object.__setattr__(self, name, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们再来测试一下Linear这个类的参数列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.w = Tensor.from_numpy(np.random.rand(in_features, out_features))\n",
    "        self.b = Tensor.from_numpy(np.random.rand(1, out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x*self.w+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w Tensor(data=[[0.28560691 0.8672992  0.88595457 0.11899527 0.80321621]\n",
      " [0.33203469 0.50307607 0.952864   0.36840301 0.2706384 ]\n",
      " [0.97381182 0.42024134 0.49264617 0.38794506 0.59529725]\n",
      " [0.08387362 0.30237794 0.26447564 0.93845391 0.16128597]\n",
      " [0.40830442 0.53959658 0.50127377 0.21927836 0.40006681]\n",
      " [0.58827379 0.74123641 0.43745427 0.13590099 0.01242012]\n",
      " [0.43069382 0.59793469 0.20412198 0.08868851 0.90757201]\n",
      " [0.90974848 0.84735987 0.02955181 0.62218034 0.69052233]\n",
      " [0.86024293 0.16520478 0.07021213 0.7879401  0.93634434]\n",
      " [0.51618515 0.21954381 0.40886429 0.09486535 0.58629152]], grad=[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]], trainable=True)\n",
      "b Tensor(data=[[0.53476514 0.53942268 0.36438901 0.44903074 0.13865316]], grad=[[0. 0. 0. 0. 0.]], trainable=True)\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in linear_layer.named_parameters():\n",
    "    print(name, parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module()\n"
     ]
    }
   ],
   "source": [
    "print(linear_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们完成\\_\\_repr\\_\\_方法，来自动打印模型的网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self._parameters = {}\n",
    "        self._modules = {}\n",
    "\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, *input):\n",
    "        return self.forward(*input)\n",
    "\n",
    "    def named_parameters(self, memo=None, prefix=''):\n",
    "        if memo is None:\n",
    "            memo = set()\n",
    "\n",
    "        for name, param in self._parameters.items():\n",
    "            if param not in memo:\n",
    "                memo.add(param)\n",
    "                yield prefix + name, param\n",
    "\n",
    "        for name, mod in self._modules.items():\n",
    "            submodule_prefix = prefix + name + '.'\n",
    "            for name, param in mod.named_parameters(memo, submodule_prefix):\n",
    "                yield name, param\n",
    "\n",
    "    def add_module(self, name, module):\n",
    "        if not isinstance(module, Module) and module is not None:\n",
    "            raise TypeError(\"{} is not a Module subclass\".format(type(module)))\n",
    "        self._modules[name] = module\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, Tensor):\n",
    "            object.__setattr__(self, name, value)  # 先设置属性\n",
    "            self._parameters[name] = value          # 然后添加到参数字典中\n",
    "        elif isinstance(value, Module):\n",
    "            object.__setattr__(self, name, value)\n",
    "            self.add_module(name, value)\n",
    "        else:\n",
    "            object.__setattr__(self, name, value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        lines = [self.__class__.__name__ + '(']\n",
    "        for name, module in self._modules.items():\n",
    "            mod_str = repr(module).replace('\\n', '\\n  ')\n",
    "            lines.append(f\"  ({name}): {mod_str}\")\n",
    "        lines.append(')')\n",
    "        return '\\n'.join(lines)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们再对Linear类进行简单的修改，让其变成我们的标准模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, bias = True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.w = Tensor.from_numpy(np.random.rand(in_features, out_features))\n",
    "        if bias:\n",
    "            self.b = Tensor.from_numpy(np.random.rand(1, out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x*self.w+self.b if self.bias else x*self.w\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Linear(in_features={self.in_features}, out_features={self.out_features}, bias={self.bias})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = Linear(5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[0.81330898 1.05057424 1.38959737 1.19471005 1.97466068 1.53463671]], grad=[[0. 0. 0. 0. 0. 0.]], trainable=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(Tensor.from_numpy(np.random.rand(1, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=6, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = Linear(5,6)\n",
    "        self.l2 = Linear(6,8)\n",
    "        self.l3 = Linear(8,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = x.relu()\n",
    "        x = self.l2(x)\n",
    "        x = x.relu()\n",
    "        x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[19.03068868 12.97512342]], grad=[[0. 0.]], trainable=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n(Tensor.from_numpy(np.random.rand(1, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (l1): Linear(in_features=5, out_features=6, bias=True)\n",
      "  (l2): Linear(in_features=6, out_features=8, bias=True)\n",
      "  (l3): Linear(in_features=8, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，让我们摩饭pytorch风格，实现MSELoss类做一个收尾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        loss = 0\n",
    "        for i in range(len(target)):\n",
    "            loss += (pred[i] - target[i]) ** 2\n",
    "        return loss/len(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们实现随机批次类，我们会在每个epoch开始的时候随机打乱整个数据集，再按批次大小从中取数，知道取完再开始下一个循环的随机打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler:\n",
    "    def __init__(self, train_x, train_y, batch_size):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(self.train_x))\n",
    "        np.random.shuffle(self.indices)  # Shuffle at the start\n",
    "        self.current_index = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        if self.current_index + self.batch_size > len(self.train_x):\n",
    "            # Reshuffle the indices and reset the current index\n",
    "            np.random.shuffle(self.indices)\n",
    "            self.current_index = 0\n",
    "\n",
    "        batch_indices = self.indices[self.current_index:self.current_index + self.batch_size]\n",
    "        self.current_index += self.batch_size\n",
    "        return self.train_x[batch_indices], self.train_y[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，让我们简单测试一下我们的程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return 4*x**3 - 5*x*x + 4*x - 5\n",
    "\n",
    "import numpy as np\n",
    "x_values = np.linspace(-10, 10, 400)\n",
    "y_values = func(x_values)\n",
    "\n",
    "# Randomly choosing training data\n",
    "random_indices = np.random.choice(len(x_values), size=320, replace=False)\n",
    "train_x = x_values[random_indices]\n",
    "train_y = y_values[random_indices]\n",
    "\n",
    "# The remaining data can be considered as the testing dataset\n",
    "test_x = np.delete(x_values, random_indices)\n",
    "test_y = np.delete(y_values, random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = Linear(1,10)\n",
    "        self.l2 = Linear(10,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = x.relu()\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[45.38756473]], grad=[[0.]], trainable=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/30000, Loss: Tensor(data=[[1.53757469e+08]], grad=[[0.]], trainable=True)\n",
      "Epoch 100/30000, Loss: Tensor(data=[[1.2935343e+08]], grad=[[0.]], trainable=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     13\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 14\u001b[0m     loss\u001b[38;5;241m.\u001b[39madam_opt(t, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_sum\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/scratch-anything/deep learning/jupyter_notebook/2. 从零开始实现_从神经元到全连接神经网络/mytorch.py:298\u001b[0m, in \u001b[0;36mTensor.adam_opt\u001b[0;34m(self, t, learning_rate, beta1, beta2, epsilon, grad_zero)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# 更新动量和速度\u001b[39;00m\n\u001b[1;32m    297\u001b[0m tensor\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;241m=\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m tensor\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1) \u001b[38;5;241m*\u001b[39m tensor\u001b[38;5;241m.\u001b[39mgrad\n\u001b[0;32m--> 298\u001b[0m tensor\u001b[38;5;241m.\u001b[39mvelocity \u001b[38;5;241m=\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m tensor\u001b[38;5;241m.\u001b[39mvelocity \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(tensor\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# 计算偏差校正后的估计\u001b[39;00m\n\u001b[1;32m    301\u001b[0m m_hat \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m t)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch = 30000\n",
    "batch_size = 80\n",
    "batch_sampler = BatchSampler(train_x, train_y, batch_size)\n",
    "mse_loss = MSELoss()\n",
    "t = 0\n",
    "for i in range(epoch):\n",
    "    loss_sum = 0\n",
    "    for _ in range(len(train_y)//5):\n",
    "        batch_train_x, batch_train_y = batch_sampler.next_batch()\n",
    "        pred = [net(x) for x in batch_train_x]\n",
    "        loss = mse_loss(pred, batch_train_y)\n",
    "        loss_sum += loss\n",
    "        loss.backward()\n",
    "        loss.adam_opt(t, learning_rate=0.001)\n",
    "    if i % 100 == 0:\n",
    "        print(f'Epoch {i}/{epoch}, Loss: {loss_sum}')\n",
    "    t += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
