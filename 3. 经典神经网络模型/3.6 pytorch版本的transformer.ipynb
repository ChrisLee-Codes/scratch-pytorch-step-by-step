{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 位置编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # d_model是词嵌入维度\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 位置编码矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 绝对位置矩阵\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)*-(math.log(10000.0)/d_model))\n",
    "        pe[:,0::2] = torch.sin(position*div_term)\n",
    "        pe[:,1::2] = torch.cos(position*div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:,:x.size(1)],requires_grad=False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = Embeddings(512,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor([[1,2,3,4],[1,3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embeddings(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positionalEncoding = PositionalEncoding(512, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_result = positionalEncoding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "pe = PositionalEncoding(20,0)\n",
    "y = pe(Variable(torch.zeros(1,100,20)))\n",
    "plt.plot(np.arange(100),y[0,:,4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 掩码张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape),k=1).astype('uint8')\n",
    "    return torch.from_numpy(1 - subsequent_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    # 先取词嵌入维度\n",
    "    d_k = query.size(-1)\n",
    "    # 把key的最后两个维度进行转置，q和k的维度是(batch, sequence_length_q, features)和(batch, sequence_length_k, features)\n",
    "    # 因此torch.matmul(query, key.transpose(-2, -1))的维度是(batch, sequence_length_q, sequence_length_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(d_k)\n",
    "    print(\"attention\", mask.shape, scores.shape)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    # 对最后一维做softmax操作\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(5,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Variable(torch.zeros(5,5))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.masked_fill(mask == 0, -1e9)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = key = value = pe_result\n",
    "mask = Variable(torch.zeros(2,4,4))\n",
    "attn, p_attn = attention(query, key, value,mask=mask)\n",
    "print('attn', attn)\n",
    "print(attn.shape)\n",
    "print('p_attn', p_attn)\n",
    "print(p_attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, head, embedding_dim, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert embedding_dim % head == 0\n",
    "        self.d_k = embedding_dim // head\n",
    "        self.head = head\n",
    "        # 多头注意力中Q, K, V需要线性层，最后多头合并后还需要线性层，因此一共需要4个线性层\n",
    "        self.linears = clones(nn.Linear(embedding_dim, embedding_dim), 4)\n",
    "        # 代码最后得到的注意力张量\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # 使用squeeze将掩码张量进行维度扩充，代表多头中的第n个头\n",
    "            mask = mask.unsqueeze(1)\n",
    "        batch_size = query.size(0)\n",
    "        # 这个地方非常重要，self.linears中的前三个矩阵就是q,k,v的权重矩阵，是被用来迭代训练的\n",
    "        query, key, value = [model(x).view(batch_size, -1, self.head, self.d_k).transpose(1,2) for model, x in zip(self.linears, (query,key,value))]\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        x = x.transpose(1,2).contiguous().view(batch_size, -1, self.head*self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = 8\n",
    "embedding_dim = 512\n",
    "dropout = 0.2\n",
    "\n",
    "query = key = value = pe_result\n",
    "mask = Variable(torch.zeros(2,4,4))\n",
    "mha = MultiHeadedAttention(head, embedding_dim, dropout)\n",
    "mha_result = mha(query, key, value, mask)\n",
    "print(mha_result)\n",
    "print(mha_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前馈全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(self.dropout(F.relu(self.w1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "d_ff = 64\n",
    "dropout = 0.2\n",
    "\n",
    "x = mha_result\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "ff_result = ff(x)\n",
    "print(ff_result)\n",
    "print(ff_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 规范化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    # eps在规范化公式中作为分母出现，为防止分母是0，设置为1e-6\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a2 = nn.Parameter(torch.ones(features))\n",
    "        self.b2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # keepdim操作让张量只在数字上进行规范化，而不会改变形状\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a2 * (x - mean) / (std + self.eps) + self.b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ff_result\n",
    "ln = LayerNorm(512)\n",
    "ln_result = ln(x)\n",
    "print(ln_result)\n",
    "print(ln_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 子层连接结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout=0.1):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # sublayer是子层连接的函数\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 512\n",
    "dropout = 0.2\n",
    "head = 8\n",
    "d_model = 512\n",
    "\n",
    "x = pe_result\n",
    "mask = Variable(torch.zeros(2,4,4))\n",
    "self_attn = MultiHeadedAttention(head, d_model)\n",
    "sublayer = lambda x: self_attn(x,x,x,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SublayerConnection(size, dropout)\n",
    "sc_result = sc(x, sublayer)\n",
    "print(sc_result)\n",
    "print(sc_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码器层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x,mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 512\n",
    "dropout = 0.2\n",
    "head = 8\n",
    "d_model = 512\n",
    "d_ff = 64\n",
    "x = pe_result\n",
    "\n",
    "self_attn = MultiHeadedAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "mask = Variable(torch.zeros(2,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el = EncoderLayer(size, self_attn, ff, dropout)\n",
    "el_result = el(x, mask)\n",
    "print(el_result)\n",
    "print(el_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 512\n",
    "dropout = 0.2\n",
    "head = 8\n",
    "d_model = 512\n",
    "d_ff = 64\n",
    "x = pe_result\n",
    "\n",
    "c = copy.deepcopy\n",
    "\n",
    "attn = MultiHeadedAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "layer = EncoderLayer(size, c(attn), c(ff), dropout)\n",
    "\n",
    "N = 8\n",
    "mask = Variable(torch.zeros(2,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = Encoder(layer, N)\n",
    "en_result = en(x, mask)\n",
    "print(en_result)\n",
    "print(en_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码器层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        # 多头自注意力对象，Q=K=V\n",
    "        self.self_attn = self_attn\n",
    "        # 这里Q!=K=V\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.dropout = dropout\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "    \n",
    "    def forward(self, x, memory, source_mask, target_mask):\n",
    "        # memory是编码器层的语义存储变量，source_mask是源数据掩码张量，\n",
    "        # target_mask是目标数据掩码张量\n",
    "        m = memory\n",
    "        # 为了将解码时，未来的信息进行遮掩，采用target_mask,比如解码时解码到第二个字符，就只能看到第一个字符的信息\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, target_mask))\n",
    "\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, source_mask))\n",
    "\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 512\n",
    "dropout = 0.2\n",
    "head = 8\n",
    "d_model = 512\n",
    "d_ff = 64\n",
    "self_attn = src_attn = MultiHeadedAttention(head, d_model, dropout)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pe_result\n",
    "memory = en_result\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "source_mask = target_mask = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DecoderLayer(size, self_attn, src_attn, ff, dropout)\n",
    "dl_result = dl(x, memory, source_mask, target_mask)\n",
    "print(dl_result)\n",
    "print(dl_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, source_mask, target_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, source_mask, target_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 512\n",
    "dropout = 0.2\n",
    "head = 8\n",
    "d_model = 512\n",
    "d_ff = 64\n",
    "c = copy.deepcopy\n",
    "attn = MultiHeadedAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "layer = DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout)\n",
    "N = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pe_result\n",
    "memory = en_result\n",
    "mask = Variable(torch.zeros(2, 4, 4))\n",
    "source_mask = target_mask = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = Decoder(layer, N)\n",
    "de_result = de(x, memory, source_mask, target_mask)\n",
    "print(de_result)\n",
    "print(de_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.project = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对最后一个维度，也就是vocab_size取softmax\n",
    "        return F.log_softmax(self.project(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "vocab_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(d_model, vocab_size)\n",
    "gen_result = gen(x)\n",
    "print(gen_result)\n",
    "print(gen_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, source_embed, target_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = source_embed\n",
    "        self.tgt_embed = target_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def encode(self, source, source_mask):\n",
    "        return self.encoder(self.src_embed(source), source_mask)\n",
    "    \n",
    "    def decode(self, memory, source_mask, target, target_mask):\n",
    "        return self.decoder(self.tgt_embed(target), memory, source_mask, target_mask)\n",
    "\n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        return self.decode(self.encode(source, source_mask), source_mask, target, target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "d_model = 512\n",
    "encoder = en\n",
    "decoder = de\n",
    "source_embed = nn.Embedding(vocab_size, d_model)\n",
    "target_embed = nn.Embedding(vocab_size, d_model)\n",
    "generator = gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设源数据与目标数据相同, 实际中并不相同\n",
    "source = target = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))\n",
    "\n",
    "# 假设src_mask与tgt_mask相同，实际中并不相同\n",
    "source_mask = target_mask = Variable(torch.zeros(2, 4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed = EncoderDecoder(encoder, decoder, source_embed, target_embed, generator)\n",
    "ed_result = ed(source, target, source_mask, target_mask)\n",
    "print(ed_result)\n",
    "print(ed_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(source_vocab, target_vocab, N=6, d_model=512, d_ff=2048, head=8, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(head, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, source_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, target_vocab), c(position)),\n",
    "        Generator(d_model, target_vocab)\n",
    "    )\n",
    "    # 模型结构完成后，接下来就是初始化模型中的参数，比如线性层中的变换矩阵\n",
    "    # 这里一但判断参数的维度大于1，则会将其初始化成一个服从均匀分布的矩阵U(-a, a)，\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = 11\n",
    "target_vocab = 11 \n",
    "N = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    res = make_model(source_vocab, target_vocab, N)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用copy任务进行模型基本测试的四步曲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入工具包Batch, 它能够对原始样本数据生成对应批次的掩码张量\n",
    "from pyitcast.transformer_utils import Batch  \n",
    "\n",
    "def data_generator(V, batch, num_batch):\n",
    "    \"\"\"\n",
    "        该函数用于随机生成copy任务的数据, 它的三个输入参数是V: 随机生成数字的最大值+1, \n",
    "        batch: 每次输送给模型更新一次参数的数据量, num_batch: 一共输送num_batch次完成一轮\n",
    "    \"\"\"\n",
    "    # 使用for循环遍历nbatches\n",
    "    for i in range(num_batch):\n",
    "        # 在循环中使用np的random.randint方法随机生成[1, V)的整数, \n",
    "        # 分布在(batch, 10)形状的矩阵中, 然后再把numpy形式转换称torch中的tensor.\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "\n",
    "        # 接着使数据矩阵中的第一列数字都为1, 这一列也就成为了起始标志列, \n",
    "        # 当解码器进行第一次解码的时候, 会使用起始标志列作为输入.\n",
    "        data[:, 0] = 1\n",
    "\n",
    "        # 因为是copy任务, 所有source与target是完全相同的, 且数据样本作用变量不需要求梯度\n",
    "        # 因此requires_grad设置为False\n",
    "        source = Variable(data, requires_grad=False)\n",
    "        target = Variable(data, requires_grad=False)\n",
    "\n",
    "        # 使用Batch对source和target进行对应批次的掩码张量生成, 最后使用yield返回\n",
    "        yield Batch(source, target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将生成0-10的整数\n",
    "V = 11\n",
    "\n",
    "# 每次喂给模型20个数据进行参数更新\n",
    "batch = 20 \n",
    "\n",
    "# 连续喂30次完成全部数据的遍历, 也就是1轮\n",
    "num_batch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    res = data_generator(V, batch, num_batch)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获得Transformer模型及其优化器和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"\"\"A simple loss compute and train function.\"\"\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1))\n",
    "        loss = loss / norm\n",
    "        if self.opt is not None:\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.item() * norm  # 修改这里，使用 .item() 获取0维张量的Python数值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyitcast.transformer_utils import get_std_opt\n",
    "from pyitcast.transformer_utils import LabelSmoothing\n",
    "\n",
    "model = make_model(V, V, N=2)\n",
    "model_optimizer = get_std_opt(model)\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "loss = SimpleLossCompute(model.generator, criterion, model_optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyitcast.transformer_utils import LabelSmoothing\n",
    "\n",
    "# 使用LabelSmoothing实例化一个crit对象.\n",
    "# 第一个参数size代表目标数据的词汇总数, 也是模型最后一层得到张量的最后一维大小\n",
    "# 这里是5说明目标词汇总数是5个. 第二个参数padding_idx表示要将那些tensor中的数字\n",
    "# 替换成0, 一般padding_idx=0表示不进行替换. 第三个参数smoothing, 表示标签的平滑程度\n",
    "# 如原来标签的表示值为1, 则平滑后它的值域变为[1-smoothing, 1+smoothing].\n",
    "crit = LabelSmoothing(size=5, padding_idx=0, smoothing=0.5)\n",
    "\n",
    "# 假定一个任意的模型最后输出预测结果和真实结果\n",
    "predict = Variable(torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]]))\n",
    "\n",
    "# 标签的表示值是0，1，2\n",
    "target = Variable(torch.LongTensor([2, 1, 0]))\n",
    "\n",
    "# 将predict, target传入到对象中\n",
    "crit(predict, target)\n",
    "\n",
    "# 绘制标签平滑图像\n",
    "plt.imshow(crit.true_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 运行模型进行训练评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyitcast.transformer_utils import run_epoch\n",
    "\n",
    "def run(model, loss, epochs=10):\n",
    "    # 遍历轮数\n",
    "    for epoch in range(epochs):\n",
    "        # 模型使用训练模式, 所有参数将被更新\n",
    "        model.train()\n",
    "        # 训练时, batch_size是20\n",
    "        run_epoch(data_generator(V, 8, 20), model, loss)\n",
    "\n",
    "        # 模型使用评估模式, 参数将不会变化 \n",
    "        model.eval()\n",
    "        # 评估时, batch_size是5\n",
    "        run_epoch(data_generator(V, 8, 5), model, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(model, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyitcast.transformer_utils import greedy_decode \n",
    "\n",
    "\n",
    "def run(model, loss, epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        run_epoch(data_generator(V, 8, 20), model, loss)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        run_epoch(data_generator(V, 8, 5), model, loss)\n",
    "\n",
    "    # 模型进入测试模式\n",
    "    model.eval()\n",
    "\n",
    "    # 假定的输入张量\n",
    "    source = Variable(torch.LongTensor([[1,3,2,5,4,6,7,8,9,10]]))\n",
    "\n",
    "    # 定义源数据掩码张量, 因为元素都是1, 在我们这里1代表不遮掩\n",
    "    # 因此相当于对源数据没有任何遮掩.\n",
    "    source_mask = Variable(torch.ones(1, 1, 10))\n",
    "\n",
    "    # 最后将model, src, src_mask, 解码的最大长度限制max_len, 默认为10\n",
    "    # 以及起始标志数字, 默认为1, 我们这里使用的也是1\n",
    "    result = greedy_decode(model, source, source_mask, max_len=10, start_symbol=1)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(model, loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
